Model name:  ViT-L/14
Input resolution: 224
Context length: 77
Vocab size: 49408
____________________________
First arg, (image source) =  ./train
Second arg, (file-label list) =  ./country100.csv
Third arg, (label type) =  country
Got batch number:  0
Traceback (most recent call last):
  File "ModelRun2.py", line 104, in <module>
    image_features = model.encode_image(image_input).float()
  File "/home/jocl/.local/lib/python3.8/site-packages/clip/model.py", line 337, in encode_image
    return self.visual(image.type(self.dtype))
  File "/home/jocl/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jocl/.local/lib/python3.8/site-packages/clip/model.py", line 228, in forward
    x = self.transformer(x)
  File "/home/jocl/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jocl/.local/lib/python3.8/site-packages/clip/model.py", line 199, in forward
    return self.resblocks(x)
  File "/home/jocl/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jocl/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/jocl/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jocl/.local/lib/python3.8/site-packages/clip/model.py", line 186, in forward
    x = x + self.attention(self.ln_1(x))
  File "/home/jocl/.local/lib/python3.8/site-packages/clip/model.py", line 183, in attention
    return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]
  File "/home/jocl/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jocl/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/jocl/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/home/jocl/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 5034, in _scaled_dot_product_attention
    attn = torch.bmm(q, k.transpose(-2, -1))
RuntimeError: CUDA out of memory. Tried to allocate 1.97 GiB (GPU 0; 7.79 GiB total capacity; 4.66 GiB already allocated; 943.94 MiB free; 5.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
